{"ast":null,"code":"'use strict';\n\nmodule.exports = text;\nfunction text(eat, value, silent) {\n  var self = this;\n  var methods;\n  var tokenizers;\n  var index;\n  var length;\n  var subvalue;\n  var position;\n  var tokenizer;\n  var name;\n  var min;\n  var now;\n\n  /* istanbul ignore if - never used (yet) */\n  if (silent) {\n    return true;\n  }\n  methods = self.inlineMethods;\n  length = methods.length;\n  tokenizers = self.inlineTokenizers;\n  index = -1;\n  min = value.length;\n  while (++index < length) {\n    name = methods[index];\n    if (name === 'text' || !tokenizers[name]) {\n      continue;\n    }\n    tokenizer = tokenizers[name].locator;\n    if (!tokenizer) {\n      eat.file.fail('Missing locator: `' + name + '`');\n    }\n    position = tokenizer.call(self, value, 1);\n    if (position !== -1 && position < min) {\n      min = position;\n    }\n  }\n  subvalue = value.slice(0, min);\n  now = eat.now();\n  self.decode(subvalue, now, handler);\n  function handler(content, position, source) {\n    eat(source || content)({\n      type: 'text',\n      value: content\n    });\n  }\n}","map":{"version":3,"names":["module","exports","text","eat","value","silent","self","methods","tokenizers","index","length","subvalue","position","tokenizer","name","min","now","inlineMethods","inlineTokenizers","locator","file","fail","call","slice","decode","handler","content","source","type"],"sources":["/home/kali/Desktop/react/React_Shopping_web/node_modules/remark-parse/lib/tokenize/text.js"],"sourcesContent":["'use strict'\n\nmodule.exports = text\n\nfunction text(eat, value, silent) {\n  var self = this\n  var methods\n  var tokenizers\n  var index\n  var length\n  var subvalue\n  var position\n  var tokenizer\n  var name\n  var min\n  var now\n\n  /* istanbul ignore if - never used (yet) */\n  if (silent) {\n    return true\n  }\n\n  methods = self.inlineMethods\n  length = methods.length\n  tokenizers = self.inlineTokenizers\n  index = -1\n  min = value.length\n\n  while (++index < length) {\n    name = methods[index]\n\n    if (name === 'text' || !tokenizers[name]) {\n      continue\n    }\n\n    tokenizer = tokenizers[name].locator\n\n    if (!tokenizer) {\n      eat.file.fail('Missing locator: `' + name + '`')\n    }\n\n    position = tokenizer.call(self, value, 1)\n\n    if (position !== -1 && position < min) {\n      min = position\n    }\n  }\n\n  subvalue = value.slice(0, min)\n  now = eat.now()\n\n  self.decode(subvalue, now, handler)\n\n  function handler(content, position, source) {\n    eat(source || content)({type: 'text', value: content})\n  }\n}\n"],"mappings":"AAAA,YAAY;;AAEZA,MAAM,CAACC,OAAO,GAAGC,IAAI;AAErB,SAASA,IAAIA,CAACC,GAAG,EAAEC,KAAK,EAAEC,MAAM,EAAE;EAChC,IAAIC,IAAI,GAAG,IAAI;EACf,IAAIC,OAAO;EACX,IAAIC,UAAU;EACd,IAAIC,KAAK;EACT,IAAIC,MAAM;EACV,IAAIC,QAAQ;EACZ,IAAIC,QAAQ;EACZ,IAAIC,SAAS;EACb,IAAIC,IAAI;EACR,IAAIC,GAAG;EACP,IAAIC,GAAG;;EAEP;EACA,IAAIX,MAAM,EAAE;IACV,OAAO,IAAI;EACb;EAEAE,OAAO,GAAGD,IAAI,CAACW,aAAa;EAC5BP,MAAM,GAAGH,OAAO,CAACG,MAAM;EACvBF,UAAU,GAAGF,IAAI,CAACY,gBAAgB;EAClCT,KAAK,GAAG,CAAC,CAAC;EACVM,GAAG,GAAGX,KAAK,CAACM,MAAM;EAElB,OAAO,EAAED,KAAK,GAAGC,MAAM,EAAE;IACvBI,IAAI,GAAGP,OAAO,CAACE,KAAK,CAAC;IAErB,IAAIK,IAAI,KAAK,MAAM,IAAI,CAACN,UAAU,CAACM,IAAI,CAAC,EAAE;MACxC;IACF;IAEAD,SAAS,GAAGL,UAAU,CAACM,IAAI,CAAC,CAACK,OAAO;IAEpC,IAAI,CAACN,SAAS,EAAE;MACdV,GAAG,CAACiB,IAAI,CAACC,IAAI,CAAC,oBAAoB,GAAGP,IAAI,GAAG,GAAG,CAAC;IAClD;IAEAF,QAAQ,GAAGC,SAAS,CAACS,IAAI,CAAChB,IAAI,EAAEF,KAAK,EAAE,CAAC,CAAC;IAEzC,IAAIQ,QAAQ,KAAK,CAAC,CAAC,IAAIA,QAAQ,GAAGG,GAAG,EAAE;MACrCA,GAAG,GAAGH,QAAQ;IAChB;EACF;EAEAD,QAAQ,GAAGP,KAAK,CAACmB,KAAK,CAAC,CAAC,EAAER,GAAG,CAAC;EAC9BC,GAAG,GAAGb,GAAG,CAACa,GAAG,CAAC,CAAC;EAEfV,IAAI,CAACkB,MAAM,CAACb,QAAQ,EAAEK,GAAG,EAAES,OAAO,CAAC;EAEnC,SAASA,OAAOA,CAACC,OAAO,EAAEd,QAAQ,EAAEe,MAAM,EAAE;IAC1CxB,GAAG,CAACwB,MAAM,IAAID,OAAO,CAAC,CAAC;MAACE,IAAI,EAAE,MAAM;MAAExB,KAAK,EAAEsB;IAAO,CAAC,CAAC;EACxD;AACF"},"metadata":{},"sourceType":"script","externalDependencies":[]}